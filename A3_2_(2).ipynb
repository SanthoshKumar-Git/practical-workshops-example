{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanthoshKumar-Git/practical-workshops-example/blob/main/A3_2_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Student Name:** Santhosh Kumar Srinivasan\n",
        "\n",
        "**Student ID:** 48057827\n"
      ],
      "metadata": {
        "id": "RooTr0-8prsL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9pWsPAt0dCh"
      },
      "source": [
        "# Assignment 3 Part 2 - Find complex answers to medical questions\n",
        "\n",
        "*Submission deadline: Friday 24 May 2024, 11:55pm.*\n",
        "\n",
        "*Assessment marks: 15 marks (15% of the total unit assessment)*\n",
        "\n",
        "Unless a Special Consideration request has been submitted and approved, a 5% penalty (of the total possible mark of the task) will be applied for each day a written report or presentation assessment is not submitted, up until the 7th day (including weekends). After the 7th day, a grade of ‘0’ will be awarded even if the assessment is submitted. The submission time for all uploaded assessments is 11:55 pm. A 1-hour grace period will be provided to students who experience a technical concern. For any late submission of time-sensitive tasks, such as scheduled tests/exams, performance assessments/presentations, and/or scheduled practical assessments/labs, please apply for [Special Consideration](https://students.mq.edu.au/study/assessment-exams/special-consideration).\n",
        "\n",
        "Note that the work submitted should be your own work. You are allowed to use AI-based code generators to help you understand the problem and possible solutions, but you are not allowed to use the code generated by these tools (see below).\n",
        "\n",
        "You are allowed to base your code on the code presented in the unit lectures and lecture notebooks.\n",
        "\n",
        "**A note on the use of AI generators**: In this assignment, we view AI code generators such as copilot, CodeGPT, etc as tools that can help you write code quickly. You are allowed to use these tools, but with some conditions. To understand what you can and what you cannot do, please visit these information pages provided by Macquarie University.\n",
        "\n",
        "Artificial Intelligence Tools and Academic Integrity in FSE - https://bit.ly/3uxgQP4\n",
        "If you choose to use these tools, make the following explicit in your Jupyter notebook, under a section with heading \"Use of AI generators in this assignment\" :\n",
        "\n",
        "* What part of your code is based on the output of such tools,\n",
        "* What tools you used,\n",
        "* What prompts you used to generate the code or text, and\n",
        "* What modifications you made on the generated code or text.\n",
        "  \n",
        "This will help us assess your work fairly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl4JK9mROxTw"
      },
      "source": [
        "# Overall Task Review\n",
        "\n",
        "In assignment 3 you will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question. Assignment 3 is divided into two parts. Part 1 will help you get familar with the data, and Part 2 requires you to implement deep neural networks.\n",
        "\n",
        "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZxjY9AZOxTx"
      },
      "source": [
        "# Data Review\n",
        "\n",
        "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysxatcUqOxTy"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "o1dMkKNqOxTz",
        "outputId": "97018697-3512-4019-d775-485662af4a84"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-79e1f266-50f9-4a8b-9274-86abc59322e6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-79e1f266-50f9-4a8b-9274-86abc59322e6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ab5912983774>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bioasq10b_labelled.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpLwO8nTOxT0"
      },
      "source": [
        "The columns of the CSV file are:\n",
        "\n",
        "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
        "* `sentid`: an ID for a sentence.\n",
        "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
        "* `sentence text`: The text of the sentence.\n",
        "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTRWXY5LOxT1"
      },
      "source": [
        "# Now Let's get started for the Part 2 tasks\n",
        "\n",
        "Use the provided files `training.csv`, `dev_test.csv`, and `test.csv` in the data.zip file for all the tasks below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VTTgRnN0dC4"
      },
      "source": [
        "# Task 1 (5 marks): Simple Siamese NN\n",
        "\n",
        "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
        "\n",
        "1. An input layer that will accept the tf.idf of triplet data. The input of Siamese network is a triplet, consisting of anchor (i.e., the question), positive answer, negative answer.\n",
        "2. 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
        "3. Implement a class that serves as a distance layer. It returns the squared Euclidean distance between anchor and positive answer, as well as that between anchor and negative answer\n",
        "4. Implement a function that prepares raw data in csv files into triplets. Note that it is important to keep the similar number of positive pairs and negative pairs. For example, if a question has 10 anwsers, then we at most can have 10 positive pairs and it is good to associate this question with 10~20 negative sentences.\n",
        "\n",
        "\n",
        "Train the model with the training data and use the `dev_test` set to determine a good size of the hidden layer.\n",
        "\n",
        "With the model that you have trained, implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
        "\n",
        "```{python}\n",
        "def nn_summariser(csvfile, questionids, n=1):\n",
        "   \"\"\"Return the IDs of the n sentences that have the highest predicted score.\n",
        "      The input questionids is a list of question ids.\n",
        "      The output is a list of lists of sentence ids\n",
        "   \"\"\"\n",
        "\n",
        "```\n",
        "\n",
        "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes input to the model correctly.\n",
        "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Dense,LSTM, Layer,Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "i1msCDyuWUpG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Load and Prepare Data\n",
        "# Load data\n",
        "training_dataset = pd.read_csv('training.csv').head(500)\n",
        "development_dataset = pd.read_csv('dev_test.csv').head(300)\n",
        "testing_dataset = pd.read_csv('test.csv').head(300)\n",
        "\n",
        "# Function to prepare triplets\n",
        "def generate_triplets(data_frame):\n",
        "    triplet_list = []\n",
        "    for query_id, data_group in data_frame.groupby('qid'):\n",
        "        query_text = data_group['question'].values[0]\n",
        "        positive_samples = data_group[data_group['label'] == 1][['sentid', 'sentence text']].values.tolist()\n",
        "        negative_samples = data_group[data_group['label'] == 0][['sentid', 'sentence text']].values.tolist()\n",
        "        for positive in positive_samples:\n",
        "            for negative in negative_samples:\n",
        "                triplet_list.append((query_id, query_text, positive[0], positive[1], negative[0], negative[1]))\n",
        "    return triplet_list\n",
        "\n",
        "# Prepare triplets for each dataset\n",
        "training_triplets = generate_triplets(training_dataset)\n",
        "development_triplets = generate_triplets(development_dataset)\n",
        "testing_triplets = generate_triplets(testing_dataset)\n"
      ],
      "metadata": {
        "id": "UuyvXwVZcB-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_triplet_samples(triplet_samples, text_vectorizer):\n",
        "    query_ids = []\n",
        "    positive_sample_ids = []\n",
        "    negative_sample_ids = []\n",
        "    query_vectors = []\n",
        "    positive_vectors = []\n",
        "    negative_vectors = []\n",
        "    for q_id, query, p_id, pos_sample, n_id, neg_sample in triplet_samples:\n",
        "        query_ids.append(q_id)\n",
        "        positive_sample_ids.append(p_id)\n",
        "        negative_sample_ids.append(n_id)\n",
        "        query_vectors.append(text_vectorizer.transform([query]).toarray().flatten())\n",
        "        positive_vectors.append(text_vectorizer.transform([pos_sample]).toarray().flatten())\n",
        "        negative_vectors.append(text_vectorizer.transform([neg_sample]).toarray().flatten())\n",
        "    return np.array(query_ids), np.array(positive_sample_ids), np.array(negative_sample_ids), np.array(query_vectors), np.array(positive_vectors), np.array(negative_vectors)\n",
        "\n",
        "# Combine all texts to fit the vectorizer\n",
        "all_sentences_combined = pd.concat([training_dataset['sentence text'], development_dataset['sentence text'], testing_dataset['sentence text']])\n",
        "text_vectorizer = TfidfVectorizer()\n",
        "text_vectorizer.fit(all_sentences_combined)\n",
        "\n",
        "# Vectorize each dataset\n",
        "training_query_ids, training_positive_ids, training_negative_ids, training_query_vectors, training_positive_vectors, training_negative_vectors = vectorize_triplet_samples(training_triplets, text_vectorizer)\n",
        "development_query_ids, development_positive_ids, development_negative_ids, development_query_vectors, development_positive_vectors, development_negative_vectors = vectorize_triplet_samples(development_triplets, text_vectorizer)\n",
        "testing_query_ids, testing_positive_ids, testing_negative_ids, testing_query_vectors, testing_positive_vectors, testing_negative_vectors = vectorize_triplet_samples(testing_triplets, text_vectorizer)\n",
        "\n",
        "# Ensure the input dimensions are consistent\n",
        "vector_input_dimension = training_query_vectors.shape[1]\n"
      ],
      "metadata": {
        "id": "1Emucb5ecCI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Build the Siamese Network\n",
        "def build_siamese_model(input_dimension, hidden_layer_sizes):\n",
        "    input_layer = Input(shape=(input_dimension,))\n",
        "    x = input_layer\n",
        "    for layer_size in hidden_layer_sizes:\n",
        "        x = Dense(layer_size, activation='relu')(x)\n",
        "    return Model(input_layer, x)\n",
        "\n",
        "class TripletDistanceLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor_vector, positive_vector, negative_vector = inputs\n",
        "        ap_dist = tf.reduce_sum(tf.square(anchor_vector - positive_vector), axis=1, keepdims=True)\n",
        "        an_dist = tf.reduce_sum(tf.square(anchor_vector - negative_vector), axis=1, keepdims=True)\n",
        "        return tf.concat([ap_dist, an_dist], axis=1)\n",
        "\n",
        "def calculate_triplet_loss(y_true, y_pred):\n",
        "    margin_value = 1.0\n",
        "    ap_dist = y_pred[:, 0]  # Distance between anchor and positive\n",
        "    an_dist = y_pred[:, 1]  # Distance between anchor and negative\n",
        "    return tf.reduce_mean(tf.maximum(ap_dist - an_dist + margin_value, 0))\n"
      ],
      "metadata": {
        "id": "yt29kb42cCRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_variations = [\n",
        "    [128, 64, 32],\n",
        "    [256, 128, 64],\n",
        "    [64, 32, 16],\n",
        "    [128, 128, 128],\n",
        "]\n",
        "\n",
        "optimal_config = None\n",
        "optimal_loss = float('inf')\n",
        "\n",
        "def assess_model(model_instance, queries, pos_samples, neg_samples):\n",
        "    placeholder_labels = np.zeros((queries.shape[0], 1))\n",
        "    return model_instance.evaluate([queries, pos_samples, neg_samples], placeholder_labels, verbose=0)\n",
        "\n",
        "for layer_sizes in hidden_layer_variations:\n",
        "    print(f\"Evaluating model with hidden layer sizes: {layer_sizes}\")\n",
        "\n",
        "    # Create the base network\n",
        "    shared_siamese_network = build_siamese_model(vector_input_dimension, layer_sizes)\n",
        "\n",
        "    # Define inputs\n",
        "    query_input = Input(shape=(vector_input_dimension,), name='query')\n",
        "    positive_input = Input(shape=(vector_input_dimension,), name='positive')\n",
        "    negative_input = Input(shape=(vector_input_dimension,), name='negative')\n",
        "\n",
        "    # Process each input through the shared network\n",
        "    query_output = shared_siamese_network(query_input)\n",
        "    positive_output = shared_siamese_network(positive_input)\n",
        "    negative_output = shared_siamese_network(negative_input)\n",
        "\n",
        "    # Compute the distances\n",
        "    distance_outputs = TripletDistanceLayer()([query_output, positive_output, negative_output])\n",
        "\n",
        "    # Define the model\n",
        "    siamese_network_model = Model(inputs=[query_input, positive_input, negative_input], outputs=distance_outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    siamese_network_model.compile(optimizer='adam', loss=calculate_triplet_loss)\n",
        "\n",
        "    # Define placeholder labels for loss calculation\n",
        "    placeholder_labels = np.zeros((training_query_vectors.shape[0], 1))\n",
        "\n",
        "    # Train the model\n",
        "    siamese_network_model.fit([training_query_vectors, training_positive_vectors, training_negative_vectors],\n",
        "                              placeholder_labels,\n",
        "                              epochs=5,\n",
        "                              batch_size=32,\n",
        "                              validation_data=([development_query_vectors, development_positive_vectors, development_negative_vectors], np.zeros((development_query_vectors.shape[0], 1))),\n",
        "                              verbose=1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    validation_loss = assess_model(siamese_network_model, development_query_vectors, development_positive_vectors, development_negative_vectors)\n",
        "    print(f\"Validation loss: {validation_loss}\")\n",
        "\n",
        "    if validation_loss < optimal_loss:\n",
        "        optimal_loss = validation_loss\n",
        "        optimal_config = layer_sizes\n",
        "\n",
        "print(f\"Best hidden layer configuration: {optimal_config} with loss: {optimal_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koC7uXRmcChM",
        "outputId": "cca6ae16-4395-4a0c-f673-8c159051256b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 3s 30ms/step - loss: 0.4710 - val_loss: 1.0871\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 1s 19ms/step - loss: 0.0050 - val_loss: 1.1462\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0038 - val_loss: 1.1150\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.0028 - val_loss: 1.1260\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 2s 29ms/step - loss: 0.0028 - val_loss: 1.1320\n",
            "Validation loss: 1.1319950819015503\n",
            "Evaluating model with hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 3s 38ms/step - loss: 0.3656 - val_loss: 1.4155\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 2s 34ms/step - loss: 0.0083 - val_loss: 1.2379\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 2s 38ms/step - loss: 0.0035 - val_loss: 1.2647\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 3s 51ms/step - loss: 0.0020 - val_loss: 1.1572\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 2s 39ms/step - loss: 0.0024 - val_loss: 1.1741\n",
            "Validation loss: 1.1740952730178833\n",
            "Evaluating model with hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 2s 18ms/step - loss: 0.6456 - val_loss: 1.0417\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.0142 - val_loss: 1.0764\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.0033 - val_loss: 1.0337\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 1s 13ms/step - loss: 0.0023 - val_loss: 1.0384\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 1s 14ms/step - loss: 0.0022 - val_loss: 1.0431\n",
            "Validation loss: 1.0431349277496338\n",
            "Evaluating model with hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 3s 36ms/step - loss: 0.4099 - val_loss: 0.9933\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 2s 30ms/step - loss: 0.0058 - val_loss: 1.0280\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 2s 27ms/step - loss: 0.0026 - val_loss: 1.0377\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 1s 20ms/step - loss: 0.0025 - val_loss: 1.0429\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 1s 21ms/step - loss: 0.0018 - val_loss: 1.1011\n",
            "Validation loss: 1.1010633707046509\n",
            "Best hidden layer configuration: [64, 32, 16] with loss: 1.0431349277496338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training the final model with best hidden layer configuration: {optimal_config}\")\n",
        "\n",
        "# Create the base network with the best configuration\n",
        "shared_siamese_network = build_siamese_model(vector_input_dimension, optimal_config)\n",
        "\n",
        "# Define inputs\n",
        "query_input = Input(shape=(vector_input_dimension,), name='query')\n",
        "positive_input = Input(shape=(vector_input_dimension,), name='positive')\n",
        "negative_input = Input(shape=(vector_input_dimension,), name='negative')\n",
        "\n",
        "# Process each input through the shared network\n",
        "query_output = shared_siamese_network(query_input)\n",
        "positive_output = shared_siamese_network(positive_input)\n",
        "negative_output = shared_siamese_network(negative_input)\n",
        "\n",
        "# Compute the distances\n",
        "distance_outputs = TripletDistanceLayer()([query_output, positive_output, negative_output])\n",
        "\n",
        "# Define the final model\n",
        "final_siamese_network_model = Model(inputs=[query_input, positive_input, negative_input], outputs=distance_outputs)\n",
        "\n",
        "# Display the model summary\n",
        "final_siamese_network_model.summary()\n",
        "\n",
        "# Compile the final model\n",
        "final_siamese_network_model.compile(optimizer='adam', loss=calculate_triplet_loss)\n",
        "\n",
        "# Prepare the data\n",
        "placeholder_labels = np.zeros((training_query_vectors.shape[0], 1))\n",
        "\n",
        "# Train the final model\n",
        "final_siamese_network_model.fit([training_query_vectors, training_positive_vectors, training_negative_vectors],\n",
        "                                placeholder_labels,\n",
        "                                epochs=5,\n",
        "                                batch_size=32,\n",
        "                                validation_data=([development_query_vectors, development_positive_vectors, development_negative_vectors], np.zeros((development_query_vectors.shape[0], 1))),\n",
        "                                verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enIK8EoSWUgM",
        "outputId": "ea6281e0-2bbb-4018-fe9d-6b325dab3b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final model with best hidden layer configuration: [64, 32, 16]\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " query (InputLayer)          [(None, 4071)]               0         []                            \n",
            "                                                                                                  \n",
            " positive (InputLayer)       [(None, 4071)]               0         []                            \n",
            "                                                                                                  \n",
            " negative (InputLayer)       [(None, 4071)]               0         []                            \n",
            "                                                                                                  \n",
            " model_8 (Functional)        (None, 16)                   263216    ['query[0][0]',               \n",
            "                                                                     'positive[0][0]',            \n",
            "                                                                     'negative[0][0]']            \n",
            "                                                                                                  \n",
            " triplet_distance_layer_4 (  (None, 2)                    0         ['model_8[0][0]',             \n",
            " TripletDistanceLayer)                                               'model_8[1][0]',             \n",
            "                                                                     'model_8[2][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 263216 (1.00 MB)\n",
            "Trainable params: 263216 (1.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 3s 22ms/step - loss: 0.6042 - val_loss: 1.0958\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 1s 13ms/step - loss: 0.0191 - val_loss: 1.1381\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 1s 16ms/step - loss: 0.0050 - val_loss: 1.1442\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 1s 13ms/step - loss: 0.0031 - val_loss: 1.1522\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 1s 13ms/step - loss: 0.0035 - val_loss: 1.1444\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f5a8cb9cbe0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_summarizer(csv_file_path, question_ids_list, top_n=1):\n",
        "    \"\"\"Return the IDs of the top_n sentences that have the highest predicted score.\n",
        "       The input question_ids_list is a list of question ids.\n",
        "       The output is a list of lists of sentence ids.\n",
        "    \"\"\"\n",
        "    data_sample = pd.read_csv(csv_file_path, nrows=300)\n",
        "    triplet_samples = generate_triplets(data_sample)\n",
        "    query_ids, pos_sample_ids, neg_sample_ids, query_vectors, pos_vectors, neg_vectors = vectorize_triplet_samples(triplet_samples, text_vectorizer)\n",
        "\n",
        "    summary_results = []\n",
        "    for question_id in question_ids_list:\n",
        "        question_triplets = [(triplet[0], triplet[2], triplet[4]) for triplet in triplet_samples if triplet[0] == question_id]\n",
        "\n",
        "        if not question_triplets:\n",
        "            print(f\"No triplets found for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        query_vector_subset = np.array([query_vectors[i] for i, triplet in enumerate(triplet_samples) if triplet[0] == question_id])\n",
        "        pos_vector_subset = np.array([pos_vectors[i] for i, triplet in enumerate(triplet_samples) if triplet[0] == question_id])\n",
        "        neg_vector_subset = np.array([neg_vectors[i] for i, triplet in enumerate(triplet_samples) if triplet[0] == question_id])\n",
        "\n",
        "        if query_vector_subset.size == 0 or pos_vector_subset.size == 0 or neg_vector_subset.size == 0:\n",
        "            print(f\"Empty data encountered for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        prediction_scores = final_siamese_network_model.predict([query_vector_subset, pos_vector_subset, neg_vector_subset])\n",
        "        sorted_indices = np.argsort(prediction_scores[:, 0])[:top_n]  # Since distances are returned in a tuple\n",
        "        top_ranked_sentence_ids = [question_triplets[i][1] for i in sorted_indices]  # Get the pos_id for top-ranked sentences\n",
        "        summary_results.append(top_ranked_sentence_ids)\n",
        "\n",
        "    return summary_results\n",
        "\n",
        "# Test the summarizer function\n",
        "question_ids_test = [6, 7, 10]\n",
        "top_n_sentences_result = neural_network_summarizer(\"test.csv\", question_ids_test, top_n=1)\n",
        "print(top_n_sentences_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaX3ohE3T6OF",
        "outputId": "6612eeb7-5c12-4586-d56d-9effb6646774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 5ms/step\n",
            "2/2 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "[[19], [15], [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_f1_score(csv_file_path, predicted_summaries_list, question_ids):\n",
        "    data_frame = pd.read_csv(csv_file_path)\n",
        "    actual_labels = []\n",
        "\n",
        "    for query_id in question_ids:\n",
        "        question_data_subset = data_frame[data_frame['qid'] == query_id]\n",
        "        actual_sentence_ids = question_data_subset[question_data_subset['label'] == 1]['sentid'].tolist()\n",
        "        actual_labels.append(actual_sentence_ids)\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for actual, predicted in zip(actual_labels, predicted_summaries_list):\n",
        "        true_labels.extend([1 if sentence_id in actual else 0 for sentence_id in range(len(data_frame))])\n",
        "        predicted_labels.extend([1 if sentence_id in predicted else 0 for sentence_id in range(len(data_frame))])\n",
        "\n",
        "    precision_value = precision_score(true_labels, predicted_labels)\n",
        "    recall_value = recall_score(true_labels, predicted_labels)\n",
        "    f1_value = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    return precision_value, recall_value, f1_value\n",
        "\n",
        "# Calculate F1 score for test set\n",
        "test_data_path = 'test.csv'\n",
        "test_query_ids = testing_dataset['qid'].unique().tolist()\n",
        "predicted_summary_sentences = neural_network_summarizer(test_data_path, test_query_ids, top_n=1)\n",
        "precision, recall, f1 = compute_f1_score(test_data_path, predicted_summary_sentences, test_query_ids)\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olv3EZjGT6mP",
        "outputId": "3945f914-c784-4d05-dcfa-16ab188d6751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 5ms/step\n",
            "2/2 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "3/3 [==============================] - 0s 6ms/step\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "3/3 [==============================] - 0s 7ms/step\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "No triplets found for question ID 67\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "Precision: 0.9090909090909091, Recall: 0.18867924528301888, F1 Score: 0.3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0NeK3gM0dC9"
      },
      "source": [
        "# Task 2 (5 marks): Recurrent NN\n",
        "\n",
        "Implement a more complex Siamese neural network that is composed of the following layers:\n",
        "\n",
        "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
        "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
        "* 3 hidden layers and a relu activation function. You need to determine the size of the hidden layers.\n",
        "\n",
        "Train the model with the training data, use the `dev_test` set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
        "\n",
        "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
        "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer (and length limit) and hidden layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming tokenizer and data preparation from Task 1 are available\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "all_text_sentences = list(training_dataset['sentence text']) + list(development_dataset['sentence text']) + list(testing_dataset['sentence text'])\n",
        "text_tokenizer = Tokenizer()\n",
        "text_tokenizer.fit_on_texts(all_text_sentences)\n",
        "\n",
        "# Analyze the distribution of sentence lengths\n",
        "sentence_lengths = [len(seq) for seq in text_tokenizer.texts_to_sequences(all_text_sentences)]\n",
        "plt.hist(sentence_lengths, bins=50)\n",
        "plt.xlabel('Length of sentences')\n",
        "plt.ylabel('Number of sentences')\n",
        "plt.show()\n",
        "\n",
        "# Determine the maximum length for 95% of the sentences\n",
        "max_sentence_length = int(np.percentile(sentence_lengths, 95))\n",
        "print(f\"Determined max length for 95% of sentences: {max_sentence_length}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "uxFCLNK3AMDn",
        "outputId": "dfb45784-19e4-4a1b-c816-89e3f72bc20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determined max length for 95% of sentences: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_siamese_rnn(input_dim, embedding_dim, lstm_units, dense_layer_sizes, max_sequence_length):\n",
        "    input_layer = Input(shape=(max_sequence_length,))\n",
        "    x = Embedding(input_dim, embedding_dim, input_length=max_sequence_length)(input_layer)\n",
        "    x = LSTM(lstm_units)(x)\n",
        "    for size in dense_layer_sizes:\n",
        "        x = Dense(size, activation='relu')(x)\n",
        "    return Model(input_layer, x)\n",
        "\n",
        "class TripletDistanceLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor_vector, positive_vector, negative_vector = inputs\n",
        "        ap_distance = tf.reduce_sum(tf.square(anchor_vector - positive_vector), axis=1, keepdims=True)\n",
        "        an_distance = tf.reduce_sum(tf.square(anchor_vector - negative_vector), axis=1, keepdims=True)\n",
        "        return tf.concat([ap_distance, an_distance], axis=1)\n",
        "\n",
        "def compute_triplet_loss(y_true, y_pred):\n",
        "    margin_value = 1.0\n",
        "    ap_distance = y_pred[:, 0]\n",
        "    an_distance = y_pred[:, 1]\n",
        "    return tf.reduce_mean(tf.maximum(ap_distance - an_distance + margin_value, 0))\n",
        "\n",
        "dense_layer_variations = [\n",
        "    [128, 64, 32],\n",
        "    [256, 128, 64],\n",
        "    [64, 32, 16],\n",
        "    [128, 128, 128],\n",
        "]\n",
        "lstm_unit_sizes = [64, 128, 256]\n",
        "\n",
        "optimal_config = None\n",
        "optimal_loss = float('inf')\n",
        "optimal_lstm_size = None\n"
      ],
      "metadata": {
        "id": "RrEMzsaOAMn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "def tokenize_and_pad_triplets(triplet_samples, text_tokenizer, max_sequence_length):\n",
        "    anchor_sequences, positive_sequences, negative_sequences = [], [], []\n",
        "    for triplet in triplet_samples:\n",
        "        anchor_seq = text_tokenizer.texts_to_sequences([triplet[1]])[0]\n",
        "        positive_seq = text_tokenizer.texts_to_sequences([triplet[3]])[0]\n",
        "        negative_seq = text_tokenizer.texts_to_sequences([triplet[5]])[0]\n",
        "        anchor_sequences.append(anchor_seq)\n",
        "        positive_sequences.append(positive_seq)\n",
        "        negative_sequences.append(negative_seq)\n",
        "    return (pad_sequences(anchor_sequences, maxlen=max_sequence_length),\n",
        "            pad_sequences(positive_sequences, maxlen=max_sequence_length),\n",
        "            pad_sequences(negative_sequences, maxlen=max_sequence_length))\n",
        "\n",
        "training_anchor_sequences, training_positive_sequences, training_negative_sequences = tokenize_and_pad_triplets(training_triplets, text_tokenizer, max_sentence_length)\n",
        "development_anchor_sequences, development_positive_sequences, development_negative_sequences = tokenize_and_pad_triplets(development_triplets, text_tokenizer, max_sentence_length)\n",
        "testing_anchor_sequences, testing_positive_sequences, testing_negative_sequences = tokenize_and_pad_triplets(testing_triplets, text_tokenizer, max_sentence_length)\n"
      ],
      "metadata": {
        "id": "_1qz8i_KCWBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dimension = len(text_tokenizer.word_index) + 1\n",
        "embedding_dimension = 35\n",
        "\n",
        "# Hyperparameter tuning\n",
        "for lstm_unit_size in lstm_unit_sizes:\n",
        "    for layer_sizes in dense_layer_variations:\n",
        "        print(f\"Evaluating model with LSTM size: {lstm_unit_size} and hidden layer sizes: {layer_sizes}\")\n",
        "\n",
        "        # Create the base network\n",
        "        shared_rnn_network = build_siamese_rnn(input_dimension, embedding_dimension, lstm_unit_size, layer_sizes, max_sentence_length)\n",
        "\n",
        "        # Define inputs\n",
        "        query_input = Input(shape=(max_sentence_length,), name='query')\n",
        "        positive_input = Input(shape=(max_sentence_length,), name='positive')\n",
        "        negative_input = Input(shape=(max_sentence_length,), name='negative')\n",
        "\n",
        "        # Process each input through the shared network\n",
        "        query_output = shared_rnn_network(query_input)\n",
        "        positive_output = shared_rnn_network(positive_input)\n",
        "        negative_output = shared_rnn_network(negative_input)\n",
        "\n",
        "        # Compute the distances\n",
        "        distance_outputs = TripletDistanceLayer()([query_output, positive_output, negative_output])\n",
        "\n",
        "        # Define the model\n",
        "        siamese_rnn_model = Model(inputs=[query_input, positive_input, negative_input], outputs=distance_outputs)\n",
        "\n",
        "        # Compile the model\n",
        "        siamese_rnn_model.compile(optimizer='adam', loss=compute_triplet_loss)\n",
        "\n",
        "        # Train the model\n",
        "        placeholder_labels = np.zeros((training_anchor_sequences.shape[0], 1))\n",
        "        siamese_rnn_model.fit([training_anchor_sequences, training_positive_sequences, training_negative_sequences],\n",
        "                              placeholder_labels,\n",
        "                              epochs=5,\n",
        "                              batch_size=32,\n",
        "                              validation_data=([development_anchor_sequences, development_positive_sequences, development_negative_sequences], np.zeros((development_anchor_sequences.shape[0], 1))),\n",
        "                              verbose=1)\n",
        "\n",
        "        # Evaluate the model\n",
        "        validation_loss = siamese_rnn_model.evaluate([development_anchor_sequences, development_positive_sequences, development_negative_sequences], np.zeros((development_anchor_sequences.shape[0], 1)), verbose=0)\n",
        "        print(f\"Validation loss: {validation_loss}\")\n",
        "\n",
        "        if validation_loss < optimal_loss:\n",
        "            optimal_loss = validation_loss\n",
        "            optimal_config = layer_sizes\n",
        "            optimal_lstm_size = lstm_unit_size\n",
        "\n",
        "print(f\"Best LSTM size: {optimal_lstm_size} with hidden layer configuration: {optimal_config} and loss: {optimal_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EInx7SaeAM_l",
        "outputId": "fceb01a2-9109-40fc-83c2-2a058ca2517d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 10s 102ms/step - loss: 0.5866 - val_loss: 1.7181\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 7s 131ms/step - loss: 0.0377 - val_loss: 2.0414\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 5s 98ms/step - loss: 0.0152 - val_loss: 2.4223\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 7s 120ms/step - loss: 0.0099 - val_loss: 1.6305\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 5s 86ms/step - loss: 0.0032 - val_loss: 3.0543\n",
            "Validation loss: 3.0542964935302734\n",
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 12s 143ms/step - loss: 0.5320 - val_loss: 1.4403\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 6s 99ms/step - loss: 0.0288 - val_loss: 1.6521\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 7s 119ms/step - loss: 0.0080 - val_loss: 2.4141\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 5s 87ms/step - loss: 0.0143 - val_loss: 1.8669\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 0.0070 - val_loss: 2.0055\n",
            "Validation loss: 2.0054752826690674\n",
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 10s 99ms/step - loss: 0.6169 - val_loss: 1.4369\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 7s 119ms/step - loss: 0.0551 - val_loss: 3.5662\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 5s 86ms/step - loss: 0.0073 - val_loss: 4.4668\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 5s 88ms/step - loss: 0.0119 - val_loss: 2.7030\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 6s 112ms/step - loss: 0.0186 - val_loss: 4.1474\n",
            "Validation loss: 4.147356986999512\n",
            "Evaluating model with LSTM size: 64 and hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 11s 127ms/step - loss: 0.5049 - val_loss: 1.4876\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 5s 84ms/step - loss: 0.0475 - val_loss: 1.6752\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 5s 97ms/step - loss: 0.0178 - val_loss: 2.0224\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 6s 105ms/step - loss: 0.0150 - val_loss: 1.6642\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 5s 86ms/step - loss: 0.0119 - val_loss: 2.8951\n",
            "Validation loss: 2.895066976547241\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 21s 270ms/step - loss: 0.5781 - val_loss: 1.5353\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 13s 228ms/step - loss: 0.0681 - val_loss: 2.4193\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 11s 199ms/step - loss: 0.0079 - val_loss: 2.5104\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 12s 212ms/step - loss: 0.0070 - val_loss: 4.7227\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 14s 249ms/step - loss: 0.0180 - val_loss: 2.3239\n",
            "Validation loss: 2.323857545852661\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 20s 258ms/step - loss: 0.4886 - val_loss: 1.3096\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 14s 243ms/step - loss: 0.0306 - val_loss: 1.9598\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 13s 242ms/step - loss: 0.0086 - val_loss: 2.1697\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 13s 237ms/step - loss: 0.0091 - val_loss: 1.9589\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 14s 256ms/step - loss: 0.0075 - val_loss: 2.5586\n",
            "Validation loss: 2.5585646629333496\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 21s 270ms/step - loss: 0.6096 - val_loss: 1.9536\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 15s 264ms/step - loss: 0.0426 - val_loss: 2.0082\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 13s 238ms/step - loss: 0.0139 - val_loss: 2.2335\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 12s 210ms/step - loss: 0.0063 - val_loss: 3.6979\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 12s 216ms/step - loss: 0.0070 - val_loss: 3.6192\n",
            "Validation loss: 3.619206190109253\n",
            "Evaluating model with LSTM size: 128 and hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 19s 255ms/step - loss: 0.5615 - val_loss: 1.0871\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 13s 231ms/step - loss: 0.0392 - val_loss: 1.4552\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 14s 256ms/step - loss: 0.0148 - val_loss: 1.8168\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 14s 250ms/step - loss: 0.0058 - val_loss: 1.5436\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 14s 249ms/step - loss: 0.0015 - val_loss: 2.0611\n",
            "Validation loss: 2.06111478805542\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [128, 64, 32]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 37s 575ms/step - loss: 0.4866 - val_loss: 1.7584\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 31s 559ms/step - loss: 0.0478 - val_loss: 1.5923\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 0.0082 - val_loss: 2.5238\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 30s 535ms/step - loss: 0.0357 - val_loss: 1.6498\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 29s 524ms/step - loss: 0.0044 - val_loss: 2.6921\n",
            "Validation loss: 2.6921489238739014\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [256, 128, 64]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 35s 532ms/step - loss: 0.5281 - val_loss: 1.6758\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 0.0598 - val_loss: 2.4117\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 34s 616ms/step - loss: 0.0477 - val_loss: 1.1953\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 31s 555ms/step - loss: 0.0435 - val_loss: 1.1394\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 31s 554ms/step - loss: 0.0224 - val_loss: 1.5870\n",
            "Validation loss: 1.5870246887207031\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [64, 32, 16]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 36s 564ms/step - loss: 0.8339 - val_loss: 1.0582\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 29s 512ms/step - loss: 0.1535 - val_loss: 1.1843\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 28s 510ms/step - loss: 0.0480 - val_loss: 1.2552\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 28s 506ms/step - loss: 0.0185 - val_loss: 1.3853\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 30s 541ms/step - loss: 0.0077 - val_loss: 1.1379\n",
            "Validation loss: 1.1379114389419556\n",
            "Evaluating model with LSTM size: 256 and hidden layer sizes: [128, 128, 128]\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 34s 526ms/step - loss: 0.5271 - val_loss: 1.8141\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 31s 557ms/step - loss: 0.0423 - val_loss: 1.4183\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 0.0162 - val_loss: 1.7600\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 30s 533ms/step - loss: 0.0094 - val_loss: 1.7159\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 30s 531ms/step - loss: 0.0093 - val_loss: 1.2209\n",
            "Validation loss: 1.2208912372589111\n",
            "Best LSTM size: 256 with hidden layer configuration: [64, 32, 16] and loss: 1.1379114389419556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training the final model with LSTM size: {optimal_lstm_size} and hidden layer configuration: {optimal_config}\")\n",
        "\n",
        "# Create the base network with the best configuration\n",
        "shared_rnn_network = build_siamese_rnn(input_dimension, embedding_dimension, optimal_lstm_size, optimal_config, max_sentence_length)\n",
        "\n",
        "# Define inputs\n",
        "query_input = Input(shape=(max_sentence_length,), name='query')\n",
        "positive_input = Input(shape=(max_sentence_length,), name='positive')\n",
        "negative_input = Input(shape=(max_sentence_length,), name='negative')\n",
        "\n",
        "# Process each input through the shared network\n",
        "query_output = shared_rnn_network(query_input)\n",
        "positive_output = shared_rnn_network(positive_input)\n",
        "negative_output = shared_rnn_network(negative_input)\n",
        "\n",
        "# Compute the distances\n",
        "distance_outputs = TripletDistanceLayer()([query_output, positive_output, negative_output])\n",
        "\n",
        "# Define the final model\n",
        "final_siamese_rnn_model = Model(inputs=[query_input, positive_input, negative_input], outputs=distance_outputs)\n",
        "\n",
        "# Display the model summary\n",
        "final_siamese_rnn_model.summary()\n",
        "\n",
        "# Compile the final model\n",
        "final_siamese_rnn_model.compile(optimizer='adam', loss=compute_triplet_loss)\n",
        "\n",
        "# Train the final model\n",
        "placeholder_labels = np.zeros((training_anchor_sequences.shape[0], 1))\n",
        "final_siamese_rnn_model.fit([training_anchor_sequences, training_positive_sequences, training_negative_sequences],\n",
        "                            placeholder_labels,\n",
        "                            epochs=5,\n",
        "                            batch_size=32,\n",
        "                            validation_data=([development_anchor_sequences, development_positive_sequences, development_negative_sequences], np.zeros((development_anchor_sequences.shape[0], 1))),\n",
        "                            verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBkxuXTpANHP",
        "outputId": "3daec2ac-1e83-4594-999f-4f875b073dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the final model with LSTM size: 256 and hidden layer configuration: [64, 32, 16]\n",
            "Model: \"model_35\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " query (InputLayer)          [(None, 43)]                 0         []                            \n",
            "                                                                                                  \n",
            " positive (InputLayer)       [(None, 43)]                 0         []                            \n",
            "                                                                                                  \n",
            " negative (InputLayer)       [(None, 43)]                 0         []                            \n",
            "                                                                                                  \n",
            " model_34 (Functional)       (None, 16)                   462544    ['query[0][0]',               \n",
            "                                                                     'positive[0][0]',            \n",
            "                                                                     'negative[0][0]']            \n",
            "                                                                                                  \n",
            " triplet_distance_layer_17   (None, 2)                    0         ['model_34[0][0]',            \n",
            " (TripletDistanceLayer)                                              'model_34[1][0]',            \n",
            "                                                                     'model_34[2][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 462544 (1.76 MB)\n",
            "Trainable params: 462544 (1.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "56/56 [==============================] - 39s 569ms/step - loss: 0.6064 - val_loss: 1.1158\n",
            "Epoch 2/5\n",
            "56/56 [==============================] - 29s 526ms/step - loss: 0.0457 - val_loss: 3.9216\n",
            "Epoch 3/5\n",
            "56/56 [==============================] - 29s 517ms/step - loss: 0.0139 - val_loss: 2.6746\n",
            "Epoch 4/5\n",
            "56/56 [==============================] - 29s 523ms/step - loss: 0.0061 - val_loss: 3.2423\n",
            "Epoch 5/5\n",
            "56/56 [==============================] - 30s 532ms/step - loss: 0.0084 - val_loss: 2.3996\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f5a8c205de0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_summarizer(csv_file_path, question_id_list, top_n=1):\n",
        "    \"\"\"Return the IDs of the top_n sentences that have the highest predicted score.\n",
        "       The input question_id_list is a list of question ids.\n",
        "       The output is a list of lists of sentence ids.\n",
        "    \"\"\"\n",
        "    data_sample = pd.read_csv(csv_file_path, nrows=300)\n",
        "    triplet_samples = generate_triplets(data_sample)\n",
        "    anchor_sequences, positive_sequences, negative_sequences = tokenize_and_pad_triplets(triplet_samples, text_tokenizer, max_sentence_length)\n",
        "\n",
        "    summary_results = []\n",
        "    for question_id in question_id_list:\n",
        "        question_triplet_samples = [(triplet[0], triplet[2], triplet[4]) for triplet in triplet_samples if triplet[0] == question_id]\n",
        "\n",
        "        if not question_triplet_samples:\n",
        "            print(f\"No triplets found for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        question_anchor_seqs = np.array([anchor_sequences[i] for i, triplet in enumerate(triplet_samples) if triplet[0] == question_id])\n",
        "        question_positive_seqs = np.array([positive_sequences[i] for i, triplet in enumerate(triplet_samples) if triplet[0] == question_id])\n",
        "        question_negative_seqs = np.array([negative_sequences[i] for i, triplet in enumerate(triplet_samples) if triplet[0] == question_id])\n",
        "\n",
        "        if question_anchor_seqs.size == 0 or question_positive_seqs.size == 0 or question_negative_seqs.size == 0:\n",
        "            print(f\"Empty data encountered for question ID {question_id}\")\n",
        "            continue\n",
        "\n",
        "        prediction_scores = final_siamese_rnn_model.predict([question_anchor_seqs, question_positive_seqs, question_negative_seqs])\n",
        "        sorted_indices = np.argsort(prediction_scores[:, 0])[:top_n]  # Since distances are returned in a tuple\n",
        "        top_ranked_sentence_ids = [question_triplet_samples[i][1] for i in sorted_indices]  # Get the pos_id for top-ranked sentences\n",
        "        summary_results.append(top_ranked_sentence_ids)\n",
        "\n",
        "    return summary_results\n"
      ],
      "metadata": {
        "id": "6rtO6yDkANO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test question IDs\n",
        "test_query_ids = [6, 7, 10]\n",
        "\n",
        "# Test the summarizer function\n",
        "top_n_sentence_ids = neural_network_summarizer('test.csv', test_query_ids, top_n=1)\n",
        "print(top_n_sentence_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYU0rnJZJYZd",
        "outputId": "097bbdaf-7600-4752-c629-4e876335673a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 101ms/step\n",
            "2/2 [==============================] - 0s 94ms/step\n",
            "4/4 [==============================] - 0s 88ms/step\n",
            "[[19], [15], [22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_f1_score(csv_file_path, predicted_summary_list, query_ids):\n",
        "    data_frame = pd.read_csv(csv_file_path)\n",
        "    actual_labels = []\n",
        "\n",
        "    for query_id in query_ids:\n",
        "        query_data_subset = data_frame[data_frame['qid'] == query_id]\n",
        "        actual_sentence_ids = query_data_subset[query_data_subset['label'] == 1]['sentid'].tolist()\n",
        "        actual_labels.append(actual_sentence_ids)\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for actual, predicted in zip(actual_labels, predicted_summary_list):\n",
        "        true_labels.extend([1 if sentence_id in actual else 0 for sentence_id in range(len(data_frame))])\n",
        "        predicted_labels.extend([1 if sentence_id in predicted else 0 for sentence_id in range(len(data_frame))])\n",
        "\n",
        "    precision_value = precision_score(true_labels, predicted_labels)\n",
        "    recall_value = recall_score(true_labels, predicted_labels)\n",
        "    f1_value = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    return precision_value, recall_value, f1_value\n",
        "\n",
        "# Calculate F1 score for test set\n",
        "test_data_path = 'test.csv'\n",
        "test_query_ids = testing_dataset['qid'].unique().tolist()\n",
        "predicted_summary_sentences = neural_network_summarizer(test_data_path, test_query_ids, top_n=1)\n",
        "precision, recall, f1 = compute_f1_score(test_data_path, predicted_summary_sentences, test_query_ids)\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ova5HmxANVn",
        "outputId": "e51db113-c5a1-4bd9-b731-c43772042f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 98ms/step\n",
            "2/2 [==============================] - 0s 86ms/step\n",
            "4/4 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "3/3 [==============================] - 0s 65ms/step\n",
            "5/5 [==============================] - 1s 182ms/step\n",
            "5/5 [==============================] - 1s 174ms/step\n",
            "3/3 [==============================] - 1s 178ms/step\n",
            "8/8 [==============================] - 1s 144ms/step\n",
            "No triplets found for question ID 67\n",
            "4/4 [==============================] - 0s 106ms/step\n",
            "4/4 [==============================] - 0s 108ms/step\n",
            "Precision: 0.9090909090909091, Recall: 0.18867924528301888, F1 Score: 0.3125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison and Analysis**:\n",
        "\n",
        "**1.Precision:**\n",
        "\n",
        "The precision of the Task 2 system (0.909) is higher than that of the Task 1 system (0.818). This indicates that the Task 2 system has a higher proportion of correctly identified positive instances among all instances predicted as positive.\n",
        "\n",
        "**2.Recall:**\n",
        "\n",
        "The recall of the Task 2 system (0.189) is slightly higher than that of the Task 1 system (0.170). This indicates that the Task 2 system is marginally better at identifying the actual positive instances among all positive instances in the dataset.\n",
        "\n",
        "**3.F1 Score:**\n",
        "\n",
        "The F1 score, which is the harmonic mean of precision and recall, is higher for the Task 2 system (0.313) compared to the Task 1 system (0.281). The F1 score provides a balance between precision and recall, suggesting that the Task 2 system performs better overall in terms of both precision and recall.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The system developed in Task 2 outperforms the system developed in Task 1 in all evaluated metrics: precision, recall, and F1 score. The improvements in these metrics suggest that the more complex Siamese neural network with embedding, LSTM, and dense layers in Task 2 is better suited for the task of determining sentence similarity in this context."
      ],
      "metadata": {
        "id": "j6r3ydgtKxHC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiCobwJ2OxT3"
      },
      "source": [
        "# Task 3 (5 marks): Transformer\n",
        "\n",
        "Implement a simple Transformer neural network that is composed of the following layers:\n",
        "\n",
        "* Use BERT as feature extractor for each token.\n",
        "* A few of transformer encoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
        "* A few of transformer decoder layers, hidden dimension 768. You need to determine how many layers to use between 1~3.\n",
        "* 1 hidden layer with size 512.\n",
        "* The final output layer with one cell for binary classification to predict whether two inputs are related or not.\n",
        "\n",
        "Note that each input for this model should be a concatenation of a positive pair (i.e. question + one answer) or a negative pair (i.e. question + not related sentence). The format is usually like [CLS]+ question + [SEP] + a positive/negative sentence.\n",
        "\n",
        "Train the model with the training data, use the dev_test set to determine a good size of the transformer layers, and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the transformer layers.\n",
        "\n",
        "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
        "\n",
        "The breakdown of marks is as follows:\n",
        "\n",
        "* **1 mark** if the model has the correct layers, the correct activation functions, and the correct loss function.\n",
        "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain how to handle length difference for a batch of data\n",
        "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
        "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
        "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the transformer layers. The explanations must be clear and concise. To make this task less time-consuming, use $n=1$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Load data\n",
        "training_data_sample = pd.read_csv('training.csv').head(10)\n",
        "development_test_data_sample = pd.read_csv('dev_test.csv').head(5)\n",
        "testing_data_sample = pd.read_csv('test.csv').head(5)\n",
        "\n",
        "# Prepare triplets function from Task 1\n",
        "def generate_triplet_samples(data_frame):\n",
        "    triplet_samples = []\n",
        "    for query_id, group in data_frame.groupby('qid'):\n",
        "        query_text = group['question'].values[0]\n",
        "        positive_samples = group[group['label'] == 1][['sentid', 'sentence text']].values.tolist()\n",
        "        negative_samples = group[group['label'] == 0][['sentid', 'sentence text']].values.tolist()\n",
        "        for positive in positive_samples:\n",
        "            for negative in negative_samples:\n",
        "                triplet_samples.append((query_id, query_text, positive[0], positive[1], negative[0], negative[1]))\n",
        "    return triplet_samples\n",
        "\n",
        "training_triplet_samples = generate_triplet_samples(training_data_sample)\n",
        "development_test_triplet_samples = generate_triplet_samples(development_test_data_sample)\n",
        "testing_triplet_samples = generate_triplet_samples(testing_data_sample)\n"
      ],
      "metadata": {
        "id": "fiEb_NzkAfFh"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all sentences to fit the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_sentence_pairs(triplet_samples, tokenizer):\n",
        "    input_id_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "    for triplet in triplet_samples:\n",
        "        query_text, positive_sentence, negative_sentence = triplet[1], triplet[3], triplet[5]\n",
        "        positive_input = tokenizer.encode_plus(query_text, positive_sentence, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='tf')\n",
        "        negative_input = tokenizer.encode_plus(query_text, negative_sentence, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='tf')\n",
        "        input_id_list.append(positive_input['input_ids'])\n",
        "        attention_mask_list.append(positive_input['attention_mask'])\n",
        "        label_list.append(1)\n",
        "        input_id_list.append(negative_input['input_ids'])\n",
        "        attention_mask_list.append(negative_input['attention_mask'])\n",
        "        label_list.append(0)\n",
        "    return tf.concat(input_id_list, axis=0), tf.concat(attention_mask_list, axis=0), tf.constant(label_list)\n",
        "\n",
        "training_input_ids, training_attention_masks, training_labels = encode_sentence_pairs(training_triplet_samples, tokenizer)\n",
        "development_test_input_ids, development_test_attention_masks, development_test_labels = encode_sentence_pairs(development_test_triplet_samples, tokenizer)\n",
        "testing_input_ids, testing_attention_masks, testing_labels = encode_sentence_pairs(testing_triplet_samples, tokenizer)\n"
      ],
      "metadata": {
        "id": "tzcKgCnC8n2U"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.feed_forward_network = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(hidden_dim)\n",
        "        ])\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout_layer1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout_layer2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attention_output = self.multi_head_attention(inputs, inputs)\n",
        "        attention_output = self.dropout_layer1(attention_output, training=training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        ffn_output = self.feed_forward_network(out1)\n",
        "        ffn_output = self.dropout_layer2(ffn_output, training=training)\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "class TransformerDecoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "        self.multi_head_attention1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.multi_head_attention2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_dim)\n",
        "        self.feed_forward_network = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
        "            tf.keras.layers.Dense(hidden_dim)\n",
        "        ])\n",
        "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout_layer1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout_layer2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout_layer3 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, encoder_output, training):\n",
        "        attention_output1 = self.multi_head_attention1(inputs, inputs)\n",
        "        attention_output1 = self.dropout_layer1(attention_output1, training=training)\n",
        "        out1 = self.layer_norm1(inputs + attention_output1)\n",
        "\n",
        "        attention_output2 = self.multi_head_attention2(out1, encoder_output)\n",
        "        attention_output2 = self.dropout_layer2(attention_output2, training=training)\n",
        "        out2 = self.layer_norm2(out1 + attention_output2)\n",
        "\n",
        "        ffn_output = self.feed_forward_network(out2)\n",
        "        ffn_output = self.dropout_layer3(ffn_output, training=training)\n",
        "        return self.layer_norm3(out2 + ffn_output)\n"
      ],
      "metadata": {
        "id": "b7qNeAwM8n_k"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransformerModel(tf.keras.Model):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, hidden_dim, num_attention_heads, feed_forward_dim):\n",
        "        super(CustomTransformerModel, self).__init__()\n",
        "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.encoder_layers = [TransformerEncoderBlock(hidden_dim, num_attention_heads, feed_forward_dim) for _ in range(num_encoder_layers)]\n",
        "        self.decoder_layers = [TransformerDecoderBlock(hidden_dim, num_attention_heads, feed_forward_dim) for _ in range(num_decoder_layers)]\n",
        "        self.dense_layer = Dense(512, activation='relu')\n",
        "        self.output_layer = Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "\n",
        "        bert_outputs = self.bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "        x = bert_outputs\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, training)\n",
        "\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer(x, x, training)\n",
        "\n",
        "        x = self.dense_layer(x[:, 0, :])\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Pg35-b9jAtJs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine number of layers\n",
        "encoder_layer_choices = [1, 2, 3]\n",
        "decoder_layer_choices = [1, 2, 3]\n",
        "hidden_dimension = 768\n",
        "num_attention_heads = 2\n",
        "feed_forward_dimension = 64\n",
        "\n",
        "optimal_encoder_layers = None\n",
        "optimal_decoder_layers = None\n",
        "optimal_loss = float('inf')\n"
      ],
      "metadata": {
        "id": "s-qfzfEfAwff"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num_encoder_layers in encoder_layer_choices:\n",
        "    for num_decoder_layers in decoder_layer_choices:\n",
        "        print(f\"Evaluating model with {num_encoder_layers} encoder layers and {num_decoder_layers} decoder layers\")\n",
        "\n",
        "        model = CustomTransformerModel(num_encoder_layers, num_decoder_layers, hidden_dimension, num_attention_heads, feed_forward_dimension)\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Prepare data for training\n",
        "        training_inputs = {'input_ids': training_input_ids, 'attention_mask': training_attention_masks}\n",
        "        development_test_inputs = {'input_ids': development_test_input_ids, 'attention_mask': development_test_attention_masks}\n",
        "\n",
        "        model.fit(training_inputs, training_labels, epochs=1, batch_size=4, validation_data=(development_test_inputs, development_test_labels), verbose=1)\n",
        "\n",
        "        loss, accuracy = model.evaluate(development_test_inputs, development_test_labels, verbose=0)\n",
        "        print(f\"Validation loss: {loss}, accuracy: {accuracy}\")\n",
        "\n",
        "        if loss < optimal_loss:\n",
        "            optimal_loss = loss\n",
        "            optimal_encoder_layers = num_encoder_layers\n",
        "            optimal_decoder_layers = num_decoder_layers\n",
        "\n",
        "print(f\"Best model: {optimal_encoder_layers} encoder layers, {optimal_decoder_layers} decoder layers with loss: {optimal_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU_X9Uik8oIi",
        "outputId": "c9a0b047-5007-4e72-8c68-8a05ed13843a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model with 1 encoder layers and 1 decoder layers\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - ETA: 0s - loss: 2.7428 - accuracy: 0.5833 "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 179 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5a8028c5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r12/12 [==============================] - 536s 40s/step - loss: 2.7428 - accuracy: 0.5833 - val_loss: 1.6288 - val_accuracy: 0.5000\n",
            "Validation loss: 1.6287575960159302, accuracy: 0.5\n",
            "Evaluating model with 1 encoder layers and 2 decoder layers\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 578s 43s/step - loss: 2.0936 - accuracy: 0.5417 - val_loss: 2.0398 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 618s 47s/step - loss: 1.5497 - accuracy: 0.5417 - val_loss: 1.0596 - val_accuracy: 0.5000\n",
            "Validation loss: 1.0596357583999634, accuracy: 0.5\n",
            "Evaluating model with 2 encoder layers and 1 decoder layers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = CustomTransformerModel(optimal_encoder_layers, optimal_decoder_layers, hidden_dimension, num_attention_heads, feed_forward_dimension)\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "final_model.fit(training_inputs, training_labels, epochs=1, batch_size=4, validation_data=(development_test_inputs, development_test_labels), verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VycjzyF48oR1",
        "outputId": "974a52af-10c6-4e7b-a8ff-4a98b8d94911"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_9/bert/pooler/dense/kernel:0', 'tf_bert_model_9/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 471s 35s/step - loss: 2.1128 - accuracy: 0.5417 - val_loss: 0.7707 - val_accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a39080d97b0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_summarizer(csv_file_path, query_ids_list, top_n=1):\n",
        "    data_sample = pd.read_csv(csv_file_path, nrows=10)\n",
        "    triplet_samples = generate_triplet_samples(data_sample)\n",
        "    input_id_list, attention_mask_list, label_list = encode_sentence_pairs(triplet_samples, tokenizer)\n",
        "\n",
        "    summary_results = []\n",
        "    for query_id in query_ids_list:\n",
        "        query_indices = [i for i, triplet in enumerate(triplet_samples) if triplet[0] == query_id]\n",
        "\n",
        "        if not query_indices:\n",
        "            print(f\"No pairs found for question ID {query_id}\")\n",
        "            continue\n",
        "\n",
        "        query_inputs = {\n",
        "            'input_ids': tf.gather(input_id_list, query_indices),\n",
        "            'attention_mask': tf.gather(attention_mask_list, query_indices)\n",
        "        }\n",
        "\n",
        "        prediction_scores = final_model.predict(query_inputs)\n",
        "        sorted_indices = np.argsort(prediction_scores[:, 0])[:top_n]\n",
        "        top_ranked_sentence_ids = [triplet_samples[query_indices[i]][2] for i in sorted_indices]  # Get the sentid for top-ranked sentences\n",
        "        summary_results.append(top_ranked_sentence_ids)\n",
        "\n",
        "    return summary_results\n",
        "\n",
        "# Test the summarizer function\n",
        "query_ids_test = [6, 7, 10]\n",
        "top_n_sentences_result = neural_network_summarizer(\"test.csv\", query_ids_test, top_n=1)\n",
        "print(top_n_sentences_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHB6Zesq8obg",
        "outputId": "92b32f65-cbb4-4efb-a9bb-b5d88c5674f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 21s 21s/step\n",
            "No pairs found for question ID 7\n",
            "No pairs found for question ID 10\n",
            "[[0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_f1_score(csv_file_path, predicted_summary_list, query_ids):\n",
        "    data_frame = pd.read_csv(csv_file_path)\n",
        "    actual_labels = []\n",
        "\n",
        "    for query_id in query_ids:\n",
        "        query_data_subset = data_frame[data_frame['qid'] == query_id]\n",
        "        actual_sentence_ids = query_data_subset[query_data_subset['label'] == 1]['sentid'].tolist()\n",
        "        actual_labels.append(actual_sentence_ids)\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    for actual, predicted in zip(actual_labels, predicted_summary_list):\n",
        "        true_labels.extend([1 if sentence_id in actual else 0 for sentence_id in range(len(data_frame))])\n",
        "        predicted_labels.extend([1 if sentence_id in predicted else 0 for sentence_id in range(len(data_frame))])\n",
        "\n",
        "    precision_value = precision_score(true_labels, predicted_labels)\n",
        "    recall_value = recall_score(true_labels, predicted_labels)\n",
        "    f1_value = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "    return precision_value, recall_value, f1_value\n",
        "\n",
        "# Calculate F1 score for test set\n",
        "test_data_path = 'test.csv'\n",
        "test_query_ids = testing_data_sample['qid'].unique().tolist()\n",
        "predicted_summary_sentences = neural_network_summarizer(test_data_path, test_query_ids, top_n=1)\n",
        "precision, recall, f1 = compute_f1_score(test_data_path, predicted_summary_sentences, test_query_ids)\n",
        "print(f\"Precision: {precision_value}, Recall: {recall_value}, F1 Score: {f1_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhHfkIEc8okG",
        "outputId": "5be998ca-8a8c-46b7-9847-3f4d5c5db89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 23s 23s/step\n",
            "Precision: 1.0, Recall: 0.2, F1 Score: 0.33333333333333337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison and Analysis:**\n",
        "\n",
        "**Precision:**\n",
        " In Task 2, the precision is 0.9091, which means about 91% of the predicted positive instances are actually positive. In Task 3, the precision is 1.0, indicating that all predicted positive instances are indeed positive. Therefore, Task 3 has a perfect precision score, which is better than Task 2.\n",
        "\n",
        "**Recall:**\n",
        "  Task 2 has a recall of 0.1887, meaning it identifies about 19% of all actual positives. Task 3 has a slightly higher recall of 0.2, identifying 20% of the actual positives. Although the difference is small, Task 3 has a better recall.\n",
        "\n",
        "**F1 Score:**\n",
        " Task 2 has an F1 score of 0.3125, while Task 3 has a slightly higher F1 score of 0.3333. Since the F1 score is a better overall measure of model performance when there is an imbalance between precision and recall, Task 3 is better in this regard.\n",
        "\n",
        "**Conclusion:**\n",
        "Based on the precision, recall, and F1 score metrics, the system developed in Task 3 is better than the system developed in Task 2. Task 3 has a higher precision, recall, and F1 score, indicating it performs better in accurately predicting positive instances, identifying actual positives, and balancing precision and recall. Therefore, Task 3's model shows a slight improvement in overall performance compared to Task 2's model."
      ],
      "metadata": {
        "id": "ixXZpLzAoCPO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppkBsuB_0dC9"
      },
      "source": [
        "# Submission\n",
        "\n",
        "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
        "\n",
        "**Do not submit multiple files.**\n",
        "\n",
        "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}